{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a120fcac-c3e2-445e-a605-107623363a9c",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT Models \n",
    "\n",
    "In this notebook, we will fine-tune a pre-trained transformer (TinyBERT) on a **token classification task**.  \n",
    "This is a common technique useful for various tasks in NLP like NER, Question-Answering etc\n",
    "\n",
    "We will:\n",
    "1. Load the required libraries and define the paths and hyperparameters.\n",
    "2. Load and prepare the dataset (`train.txt`).\n",
    "3. Load a pre-trained tiny BERT model\n",
    "4. Preprocess the data and align it with the standard format.\n",
    "5. Define the model and set the parameters.\n",
    "6. Fine-tune the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee42ba-32cb-4aa6-8aab-f8f0aea22d4f",
   "metadata": {},
   "source": [
    "Select the **bert-env** kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7ead5",
   "metadata": {},
   "source": [
    "If you do not have the bert-env kernel, you can open up a terminal on Nova and paste the following:\n",
    "\n",
    "\n",
    "\n",
    "conda create -n bert-env python=3.10 -y\n",
    "\n",
    "conda activate bert-env\n",
    "\n",
    "pip install -U transformers datasets accelerate evaluate scikit-learn\n",
    "\n",
    "pip install ipykernel\n",
    "\n",
    "pip install -U ipywidgets\n",
    "\n",
    "Then once that is done:\n",
    "\n",
    "python -m ipykernel install --user --name=bert-env --display-name \"Python (bert-env)\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "After this you can select/change the kernel to bert-env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489efec9-bb27-4b4e-9c94-348e0de0901c",
   "metadata": {},
   "source": [
    "We import all the required libraries for training. We make use of the famous Hugging-face **transformers** libray. You can also look up the documentation for transformers and pytorch for the code. The train and text files are from Professor Li`s research group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e474181-79cb-4f77-a29e-395680514944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, random, numpy as np, torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb68a89-795a-49f3-980a-e10b8b4e6968",
   "metadata": {},
   "source": [
    "We now define the **paths** of the training file,the output directory where the model will be saved and the base model path. I have already uploaded tinybert to the class folder. An alternative approach is to download it directly from the hugging face site. \n",
    "\n",
    "In case you need to use another model in your work, you can upload or reference the path of your model in the script or use hugging face to download the model.\n",
    "\n",
    "The label mapping is something specific to this training script. This might not be required if you are using another train/test file.\n",
    "\n",
    "Tiny BERT is a lightweight model which is 7x smaller than BERT but 9x faster. It is ideal for demonstration purposes. This is often used as a student model. You can read **TinyBERT: Distilling BERT for Natural Language Understanding (EMNLP findings 2020)** for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1faa1ef-d748-42bf-bc7f-cdf9c6ecf315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and settings\n",
    "DATA_PATH = \"./train.txt\"           # training file( already uploaded to class folder)\n",
    "OUTPUT_DIR = \"./ner-model\"          # fine-tuned model local save path \n",
    "MODEL_NAME = \"./tinybert-local\"     # local TinyBERT (Already uploaded to class folder)\n",
    "\n",
    "# Label mapping\n",
    "id2label = {0: \"O\", 1: \"B-Trait\"}\n",
    "label2id = {\"O\": 0, \"B-Trait\": 1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8aba7e-e43c-48c0-b3d5-045f5676e24d",
   "metadata": {},
   "source": [
    "We define the **hyperparameters** to train the model. \n",
    "\n",
    "**The hyperparameters are like the variables that control how the model learns.**\n",
    "\n",
    "We use fairly standard settings and keep the epochs low to reduce training time for the demonstration. You can change this settings around as per your requirements.\n",
    "\n",
    "Here, **EPOCHS** is how many passes or how many times the model goes through your dataset(ie the train)\n",
    "\n",
    "**TRAIN_BS** stands for training batch size. This is the number of examples the model processes before updating the weights of the model. Larger values means more faster training but more memory required to train.\n",
    "\n",
    "**EVAL_BS** stands for evaluation batch size. The number of examples processed at once when evaluating on the validation set.\n",
    "\n",
    "**MAX_LEN** stands for maximum number of tokens per sentence considered for training. Longer sentences are truncated, shorter ones are padded.\n",
    "\n",
    "**LEARN_RATE** determines the step size when updating the model weight. It is the rate of learning or how quickly the model learns. Too high means model may not converge, too low might lead to lot of time for model to converge.\n",
    "\n",
    "**VAL_SPLIT** stands for validation split. The percentage of data used to validate the model performance.The torch trainer does not use the validation directly but it is used to save the best checkpoint or the best performing model.\n",
    "\n",
    "The other parameters are specific to the environment or server we are running.\n",
    "\n",
    "Generally, the most important hyperparamerts are epochs, batch size, learning rate and max len. We also have other parameters like dropout rate etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333ddf7e-82ff-450b-b43e-66956998433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "TRAIN_BS = 8\n",
    "EVAL_BS = 8\n",
    "MAX_LEN = 64\n",
    "LEARN_RATE = 3e-5\n",
    "VAL_SPLIT = 0.1\n",
    "THREADS = max(1, os.cpu_count() // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ab78a-63de-4ca9-afe2-61446d8c3fe7",
   "metadata": {},
   "source": [
    "This code is only to ensure we are doing CPU training as we have less resources. You can choose GPU training by commenting out the os.environ line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee57a2a-2d0a-4b22-a4d2-746db51d9cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU only (remove/comment if you want GPU)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "torch.set_num_threads(THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d2887-c862-4f1c-bac2-861039a3ccda",
   "metadata": {},
   "source": [
    "We read our data using the **CoNLL** format which is a widely used standard in NLP tasks. Each line represents a single word(token) with a series of tab-separated fields. Every sentence is separated from the next by an empty line. Each word is annotated using a lable like 0/1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5111cb7-3a84-4acd-aa86-7f301fb53efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1480 sentences\n"
     ]
    }
   ],
   "source": [
    "def read_conll(path):\n",
    "    tokens_all, labels_all = [], []\n",
    "    sent_tokens, sent_labels = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line:\n",
    "                if sent_tokens:\n",
    "                    tokens_all.append(sent_tokens); labels_all.append(sent_labels)\n",
    "                    sent_tokens, sent_labels = [], []\n",
    "                continue\n",
    "            parts = re.split(r\"\\s+\", line)\n",
    "            if len(parts) >= 2:\n",
    "                tok, lab = parts[0], parts[-1]\n",
    "                sent_tokens.append(tok)\n",
    "                sent_labels.append(label2id.get(lab, 0))  # map labels to ints\n",
    "    if sent_tokens:\n",
    "        tokens_all.append(sent_tokens); labels_all.append(sent_labels)\n",
    "    return tokens_all, labels_all\n",
    "\n",
    "tokens_all, labels_all = read_conll(DATA_PATH)\n",
    "print(f\"Loaded {len(tokens_all)} sentences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af1262-5e14-4337-aec4-9f9e5964ab08",
   "metadata": {},
   "source": [
    "We preprocess the data for our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec8b963-ed8e-4bde-b5c8-c897e7e745d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'labels'],\n",
      "        num_rows: 1332\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'labels'],\n",
      "        num_rows: 148\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def build_datasets(tokens_all, labels_all, val_split=0.1, seed=42):\n",
    "    data = [{\"tokens\": t, \"labels\": l} for t, l in zip(tokens_all, labels_all)]\n",
    "    random.Random(seed).shuffle(data)\n",
    "    n_val = max(1, int(len(data) * val_split))\n",
    "    return DatasetDict({\n",
    "        \"train\": Dataset.from_list(data[n_val:]),\n",
    "        \"validation\": Dataset.from_list(data[:n_val])\n",
    "    })\n",
    "\n",
    "ds = build_datasets(tokens_all, labels_all, VAL_SPLIT)\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf19f64-0efa-4161-b418-4d1cea5ae1ac",
   "metadata": {},
   "source": [
    "We have to align and tokenize the dataset as per our training needs.This is generic boiler plate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cc6cc49-04e6-4cba-921c-fefd1105005c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0c8f4260664bebb30d99c212cc6114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2c17728421418eaafdcd3c1e3a3bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/148 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1332\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 148\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align(ds, tokenizer, max_length):\n",
    "    def _map(batch):\n",
    "        enc = tokenizer(batch[\"tokens\"], is_split_into_words=True,\n",
    "                        truncation=True, max_length=max_length)\n",
    "        aligned = []\n",
    "        for i, labels in enumerate(batch[\"labels\"]):\n",
    "            word_ids = enc.word_ids(batch_index=i)\n",
    "            prev = None\n",
    "            out = []\n",
    "            for wid in word_ids:\n",
    "                if wid is None:\n",
    "                    out.append(-100)\n",
    "                elif wid != prev:\n",
    "                    out.append(int(labels[wid]))\n",
    "                else:\n",
    "                    out.append(-100)\n",
    "                prev = wid\n",
    "            aligned.append(out)\n",
    "        enc[\"labels\"] = aligned\n",
    "        return enc\n",
    "    return ds.map(_map, batched=True, remove_columns=[\"tokens\",\"labels\"])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "ds_tok = tokenize_and_align(ds, tokenizer, MAX_LEN)\n",
    "print(ds_tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf62a366-7388-443e-a769-f7d7d070259f",
   "metadata": {},
   "source": [
    "We define the model parameters. We are using the model for **token classification** and also choose our mapping,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f65342c-89e4-4736-8a9f-1e733d2def03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1254f4da-4557-4011-be7e-6fdc64ee5af1",
   "metadata": {},
   "source": [
    "Some boiler plate code for training. You can remove the compute metrics part in case you want it. However the args refers to the Training arguments. We set the learning rate which is a very important parameter and also load our previously defined hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58c9b7eb-e4ab-45a6-80b5-05444272482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=-1).flatten()\n",
    "    labels = p.label_ids.flatten()\n",
    "\n",
    "    # remove padding (-100)\n",
    "    valid = labels != -100\n",
    "    preds = preds[valid]\n",
    "    labels = labels[valid]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c425e",
   "metadata": {},
   "source": [
    "We now apply the **arguments** to the args variable which sort of takes the parameters for training. Most of the arguments are the hyperparameters we have already defined. Some of the values are generic trivial or default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e16f2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LEARN_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed85e40",
   "metadata": {},
   "source": [
    "We define the **trainer** and set the arguments for it as per what we have already defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d12f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_262719/2209538232.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, args=args,\n",
    "    data_collator=collator, tokenizer=tokenizer,\n",
    "    train_dataset=ds_tok[\"train\"], eval_dataset=ds_tok[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7998e4e1-e9b9-4875-a75c-0d6e92f7de26",
   "metadata": {},
   "source": [
    "Finally we are ready to **train** our model. I have kept a print statement to show what is the step and training loss. Notice how the training loss reduces with more steps. Ignore any warnings if displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "000a82b9-10f2-4ee7-92ea-6391366b5494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/LAS/qli-lab/pmaitra/miniconda3/envs/bert-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='334' max='334' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [334/334 03:57, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.103500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/LAS/qli-lab/pmaitra/miniconda3/envs/bert-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: {'eval_loss': 0.004390090238302946, 'eval_accuracy': 1.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 2.2419, 'eval_samples_per_second': 66.014, 'eval_steps_per_second': 8.475, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pmaitra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/pmaitra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/pmaitra/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "eval_out = trainer.evaluate()\n",
    "print(\"Eval:\", eval_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e5357-c635-492f-8d02-d6594a5d56a9",
   "metadata": {},
   "source": [
    "The final step is to **save** the fine-tuned model in our output directory. The model has now been trained for token classification on the training text file. We can later use the saved model for inference or prediction on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1a532c5-8fd3-48e9-b74a-f30152dc2a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to ./ner-model\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR,\"run_report.json\"),\"w\") as f:\n",
    "    json.dump({\"metrics_eval\": eval_out}, f, indent=2)\n",
    "\n",
    "print(f\"✅ Model saved to {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c7428f",
   "metadata": {},
   "source": [
    "A few things to note. The model is a very tiny one and I chose it as such for demonstration purposes. Also, our training text is too small. \n",
    "\n",
    "You can scale up the model by using the actual BERT model or other variants like RoBERTa or DistilBERT. Also you can increase the training dataset size and also change the hyperparameters like the number of epochs, batch size etc to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68712d-e0b2-41e9-8cd9-71b8c0a54186",
   "metadata": {},
   "source": [
    "<h1>THANK YOU</h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-env)",
   "language": "python",
   "name": "bert-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
