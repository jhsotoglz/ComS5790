{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1. basic string operations\n",
    "sentence = \"Iowa State University, located in Ames, is a renowned public research university.\"\n",
    "print(\"[example sentence]: \" + sentence)\n",
    "print()\n",
    "\n",
    "# 1.1 convert to uppercase/lowercase\n",
    "uppercase = sentence.upper()\n",
    "lowercase = sentence.lower()\n",
    "print(\"Uppercase:\", uppercase)\n",
    "print(\"Lowercase:\", lowercase)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1.2 split into words & join words\n",
    "words = sentence.split()\n",
    "print(\"Words in the sentence:\", words)\n",
    "print()\n",
    "\n",
    "joined_sentence = \" \".join(words)\n",
    "print(\"Joined Sentence:\", joined_sentence)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1.3 find substrings & replace substrings\n",
    "index = sentence.find(\"Ames\") \n",
    "# returns the index of the first occurrence of the substring. \n",
    "# if the substring is not found, it returns -1\n",
    "print(f\"'Ames' found at index: {index}\")\n",
    "print()\n",
    "\n",
    "modified_sentence = sentence.replace(\"Ames\", \"Ames, Iowa\")\n",
    "print(\"Modified Sentence:\", modified_sentence)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1.4 access characters by Index\n",
    "first_char = sentence[0]\n",
    "last_char = sentence[-1]\n",
    "print(f\"First Character: {first_char}\")\n",
    "print(f\"Last Character: {last_char}\")\n",
    "print()\n",
    "\n",
    "substring = sentence[0:21]  # \"Iowa State University\", the blank space also counts\n",
    "print(\"Substring (0:21):\", substring)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. NLTK. \"The Natural Language Toolkit\"\n",
    "import nltk\n",
    "nltk.download('punkt')  # ensure tokenizer resources are available\n",
    "\n",
    "sentence = \"Iowa State University, located in Ames, is a renowned public research university.\"\n",
    "\n",
    "# 2.1 Tokenization\n",
    "# Word\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(sentence)\n",
    "print(\"Word Tokens:\", tokens)\n",
    "print()\n",
    "\n",
    "# Sentence\n",
    "from nltk.tokenize import sent_tokenize\n",
    "long_introduction = \"Iowa State University (ISU), in Ames, Iowa, is a top public research institution founded in 1858. \\\n",
    "Renowned for science, engineering, and agriculture, it hosts the U.S. Department of Energy’s Ames Laboratory. \\\n",
    "With over 36,000 students, ISU fosters innovation and global impact.\"\n",
    "sentences = sent_tokenize(long_introduction)\n",
    "print(\"Sentence Tokens:\", sentences)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2.2 Stop Words Removal & Frequency Distribution\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"NLTK stopwords:\", stop_words)\n",
    "print()\n",
    "filtered_words = [word for word in word_tokenize(sentence) if word.lower() not in stop_words]\n",
    "print(\"Filtered Words (No Stop Words):\", filtered_words)\n",
    "print()\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "filtered_words = [word.lower() for word in filtered_words]\n",
    "freq_dist = FreqDist(filtered_words)\n",
    "print(\"Frequency Distribution:\")\n",
    "print(freq_dist.most_common(5))\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. spaCy\n",
    "sentence = \"Iowa State University, located in Ames, is a renowned public research university.\"\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # load the small English model\n",
    "# spaCy also comes with pre-trained models for multiple languages,\n",
    "# allowing us to get started quickly with real-world text analysis\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# 3.1 Tokenization\n",
    "print(\"Tokens:\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3.2 Part-of-Speech (POS) Tagging\n",
    "print(\"POS Tags:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.pos_} ({token.tag_})\")\n",
    "# token.pos_: The simplified part-of-speech tag (e.g., NOUN, VERB, etc.).\n",
    "# token.tag_: The fine-grained POS tag, which provides more specific grammatical details (e.g., VBN for past participle verb).\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3.3 Named Entity Recognition (NER)\n",
    "print(\"Named Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} -> {ent.label_} ({spacy.explain(ent.label_)})\")\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3.4 Dependency Parsing\n",
    "# Focuses on relationships between words in a sentence.\n",
    "# Represents sentences as a directed graph where words are nodes, and dependencies (like subject-verb, object-verb) are edges.\n",
    "print(\"Dependency Parsing:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.dep_} (Head: {token.head.text})\")\n",
    "print()\n",
    "\n",
    "# this is just a simple parser from spaCy\n",
    "# there are many more advanced parsers, for example, you can try the demo: https://corenlp.run/\n",
    "# parsing bridges the gap between raw text and its syntactic or semantic meaning, making it essential for advanced language understanding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3.5 Sentence Segmentation\n",
    "long_introduction = \"Iowa State University (ISU), in Ames, Iowa, is a top public research institution founded in 1858. \\\n",
    "Renowned for science, engineering, and agriculture, it hosts the U.S. Department of Energy’s Ames Laboratory. \\\n",
    "With over 36,000 students, ISU fosters innovation and global impact.\"\n",
    "doc_long = nlp(long_introduction)\n",
    "\n",
    "print(\"Sentences:\")\n",
    "for sent in doc_long.sents:\n",
    "    print(sent.text)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3.6 Lemmatization\n",
    "# reduce a word to its base or root form (known as the \"lemma\")\n",
    "print(\"Lemmatized Tokens:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.lemma_}\")\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3.7 Similarity Between Words\n",
    "word1 = nlp(\"research\")\n",
    "word2 = nlp(\"university\")\n",
    "similarity = word1.similarity(word2)\n",
    "print(f\"Similarity between '{word1}' and '{word2}': {similarity}\")\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. Regular Expression (Regex)\n",
    "import re\n",
    "\n",
    "# 4.1 Check if a Pattern Exists\n",
    "pattern = r\"Ames\"\n",
    "match = re.search(pattern, sentence)  # stops after finding the first match in the string\n",
    "if match:\n",
    "    print(f\"Pattern '{pattern}' found at position: {match.start()}\")\n",
    "else:\n",
    "    print(f\"Pattern '{pattern}' not found\")\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4.2 Find all Case-Insensitive Matching\n",
    "pattern = r\"university\"\n",
    "matches = re.findall(pattern, sentence, re.IGNORECASE)\n",
    "print(f\"Case-insensitive matches for '{pattern}':\", matches)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4.3 Split String Using a Pattern\n",
    "pattern = r\",|\\.\"  # The pipe symbol \"|\" means \"or\" in regex; \"\\.\" matches the dot \".\"\n",
    "parts = re.split(pattern, sentence)\n",
    "print(\"Split Sentence:\", parts)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4.4 Validate Patterns (e.g., Email-Like Text)\n",
    "test_string = \"Contact me at qingwang@iastate.edu\"\n",
    "\n",
    "pattern = r\"\\b[A-Za-z0-9._-]+@[A-Za-z0-9._-]+\\.[A-Za-z]{2,}\\b\"\n",
    "# \\b\n",
    "# Matches a word boundary, ensuring the email address is a standalone word and not part of a larger string.\n",
    "# This is used at both the start and end of the pattern.\n",
    "\n",
    "# [A-Za-z0-9._-]+\n",
    "# Matches the local part of the email address (before the @).\n",
    "# Allows any combination of uppercase and lowercase letters (A-Za-z), digits (0-9), dots (.), underscores (_), and dashes (-).\n",
    "# The \"+\" ensures there is at least one character.\n",
    "\n",
    "# [A-Za-z]{2,}\n",
    "# Matches the top-level domain (e.g., com, org, net).\n",
    "# Accepts at least two characters ({2,}) and ensures they are only uppercase (A-Z) or lowercase (a-z) letters.\n",
    "\n",
    "if re.search(pattern, test_string):\n",
    "    print(\"Valid email found!\")\n",
    "else:\n",
    "    print(\"No valid email found.\")\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# 5.1 Prepare Input for BERT\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentence = \"Iowa State University, located in Ames, is a renowned public research university.\"\n",
    "\n",
    "# Tokenize the input sentence\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Convert tokens to input IDs\n",
    "input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "print(\"Input IDs:\", input_ids)\n",
    "print(\"Decoded Sentence:\", tokenizer.decode(input_ids))  \n",
    "# with special tokens [CLS] and [SEP] used for classification tasks and indicating sentence boundaries\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "print(\"Input Tensor Keys:\", inputs.keys())  # 'input_ids' and 'attention_mask'\n",
    "print(\"Input IDs Tensor:\", inputs[\"input_ids\"])\n",
    "print(\"Attention Mask Tensor:\", inputs[\"attention_mask\"])\n",
    "# the mask indicates which tokens should be attended to (1) and which should be ignored (0)\n",
    "# In this example, all tokens have a mask value of 1, meaning all tokens should be attended to\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5.2 Get BERT Output\n",
    "\n",
    "# Pass the input through BERT\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Outputs contain 'last_hidden_state' and 'pooler_output'\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output\n",
    "\n",
    "print(\"Last Hidden State Shape:\", last_hidden_state.shape)  # (batch_size, seq_len, hidden_size)\n",
    "print(\"Pooled Output Shape:\", pooled_output.shape)  # (batch_size, hidden_size)\n",
    "print()\n",
    "\n",
    "# The pooler_output can be used as a fixed-size embedding for the sentence:\n",
    "sentence_embedding = pooled_output.squeeze(0)  # Remove batch dimension\n",
    "print(\"Sentence Embedding (768-dim):\", sentence_embedding)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5.3 Token-Level Embeddings\n",
    "\n",
    "# Extract embeddings for each token\n",
    "token_embeddings = last_hidden_state.squeeze(0)  # Remove batch dimension\n",
    "print(\"Token Embeddings Shape:\", token_embeddings.shape)  # (seq_len, hidden_size)\n",
    "\n",
    "# Example: Embedding for the first token\n",
    "print(\"First Token Embedding:\", token_embeddings[0])\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5.4 Compute Sentence Similarity\n",
    "\n",
    "# Encode two sentences and compute their similarity\n",
    "sentence2 = \"Ames is home to Iowa State University, a prominent research institution.\"\n",
    "inputs2 = tokenizer(sentence2, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "# Get embeddings for both sentences\n",
    "outputs1 = model(**inputs)\n",
    "outputs2 = model(**inputs2)\n",
    "\n",
    "embedding1 = outputs1.pooler_output\n",
    "embedding2 = outputs2.pooler_output\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_similarity = torch.nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "print(\"Cosine Similarity between sentences:\", cosine_similarity.item())\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_course)",
   "language": "python",
   "name": "nlp_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}